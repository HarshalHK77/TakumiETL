{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88909323-3132-4908-b75d-38852a54733e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Stop existing Spark session if it exists\n",
    "if SparkSession.getActiveSession():\n",
    "    SparkSession.getActiveSession().stop()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Takumi ETL\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .config(\"spark.driver.memory\", \"5g\").config(\"spark.executor.memory\", \"3g\").config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.hadoop.io.nativeio.disable\", \"true\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a31a96df-7ba5-47ef-8c2d-132ca95fa2ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "id": "cSuhKeCPFB1q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time for Input_data Reading: 8.347760 seconds\n"
     ]
    }
   ],
   "source": [
    "date = \"2025-03-25\"\n",
    "start_time = time.time()\n",
    "file_path = f\"input_data/input_data_2025-03-25/input_data_2025-03-25_batch_1.parquet\"\n",
    "\n",
    "input_data = spark.read.parquet(file_path)\n",
    "symbol_data = spark.read.parquet(f\"reference_market_data/ref_market_data_{date}.parquet\")\n",
    "currency_data = spark.read.parquet(\"reference_data/ref_currency_data.parquet\")\n",
    "exchange_data = spark.read.parquet(\"reference_data/ref_exchange_data.parquet\")\n",
    "order_types_data = spark.read.parquet(\"reference_data/ref_order_types_data.parquet\")\n",
    "sides_data = spark.read.parquet(\"reference_data/ref_sides_data.parquet\")\n",
    "transaction_types_data = spark.read.parquet(\"reference_data/ref_transaction_types_data.parquet\")\n",
    "order_statuses_data = spark.read.parquet(\"reference_data/ref_order_statuses_data.parquet\")\n",
    "mics_data = spark.read.parquet(\"reference_data/ref_mics_data.parquet\")\n",
    "timing_data = spark.read.parquet(\"reference_data/ref_market_timing_data.parquet\")\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution Time for Input_data Reading: {execution_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0eb39d74-66f6-4d74-836e-54338cf2534f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QqrmdVkOLKTy",
    "outputId": "c82f8989-8516-470f-98ea-4258fe1d9173"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "Execution Time for Validating data: 3.932632 seconds\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import time\n",
    "import os\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "\n",
    "class Validator:\n",
    "    def __init__(self, df, config_path):\n",
    "        self.df = df.withColumn(\"validation_flag\", F.lit(\"\"))\n",
    "\n",
    "        # Load config\n",
    "        with open(config_path, \"r\") as file:\n",
    "            self.config = yaml.safe_load(file)\n",
    "\n",
    "        # Define expected data types\n",
    "        dtype_map = {\n",
    "            \"string\": StringType(),\n",
    "            \"double\": DoubleType()\n",
    "        }\n",
    "        self.expected_dtypes = {\n",
    "            col: dtype_map[self.config[\"validation\"][\"expected_dtypes\"][col]]\n",
    "            for col in self.config[\"validation\"].get(\"expected_dtypes\", {})\n",
    "            if self.config[\"validation\"][\"expected_dtypes\"][col] in dtype_map\n",
    "        }\n",
    "\n",
    "    def add_flag(self, condition, issue):\n",
    "        \"\"\"Appends an issue to the validation_flag column.\"\"\"\n",
    "        self.df = self.df.withColumn(\n",
    "            \"validation_flag\",\n",
    "            F.when(condition, F.concat_ws(\", \", F.col(\"validation_flag\"), F.lit(issue)))\n",
    "            .otherwise(F.col(\"validation_flag\"))\n",
    "        )\n",
    "\n",
    "    def check_missing_values(self):\n",
    "        \"\"\"Checks for missing values in required columns.\"\"\"\n",
    "        for col in self.config[\"validation\"].get(\"required_columns\", []):\n",
    "            self.add_flag(F.col(col).isNull(), f\"{col}_missing\")\n",
    "\n",
    "    def check_data_types(self):\n",
    "        \"\"\"Validates column data types.\"\"\"\n",
    "        for col, expected_type in self.expected_dtypes.items():\n",
    "            if col in self.df.columns:\n",
    "                self.add_flag(\n",
    "                    F.col(col).cast(expected_type) != F.col(col), f\"{col}_dtype_mismatch\"\n",
    "                )\n",
    "\n",
    "    def fix_categorical(self):\n",
    "        \"\"\"Replaces invalid categorical values with a default value.\"\"\"\n",
    "        for col, settings in self.config[\"validation\"].get(\"categorical\", {}).items():\n",
    "            valid_values = settings[\"valid_values\"]\n",
    "            default = settings[\"default\"]\n",
    "            valid_values_expr = F.when(F.col(col).isin(valid_values), F.col(col)).otherwise(default)\n",
    "            self.df = self.df.withColumn(col, valid_values_expr)\n",
    "\n",
    "    def fix_regex(self):\n",
    "        \"\"\"Validates a column against a regex pattern and replaces invalid values.\"\"\"\n",
    "        for col, settings in self.config[\"validation\"].get(\"regex\", {}).items():\n",
    "            pattern = settings[\"pattern\"]\n",
    "            replacement = settings[\"replacement\"]\n",
    "            mask = F.col(col).rlike(pattern)\n",
    "            self.df = self.df.withColumn(col, F.when(mask, F.col(col)).otherwise(replacement))\n",
    "\n",
    "    def run_validations(self):\n",
    "        \"\"\"Executes all validation steps.\"\"\"\n",
    "        self.check_missing_values()\n",
    "        self.check_data_types()\n",
    "        self.fix_categorical()\n",
    "        self.fix_regex()\n",
    "        return self.df\n",
    "\n",
    "# Usage Example:\n",
    "start_time = time.time()\n",
    "config_path = \"configurations/validation_configurations.yaml\"  # Path to the YAML file\n",
    "validator = Validator(input_data, config_path)\n",
    "validated_data = validator.run_validations()\n",
    "validated_data.count()\n",
    "print(validated_data.count())\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate execution time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution Time for Validating data: {execution_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1652c7c8-133d-4321-b05f-a637b6ee3780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2mlFfnuieTHV",
    "outputId": "25236320-c999-4eab-dbe2-6e5be9635a97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output data written successfully.\n",
      "Execution Time for output data generation: 6.70 seconds\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import row_number, current_timestamp, col\n",
    "from pyspark.sql.window import Window\n",
    "import os \n",
    "import time\n",
    "\n",
    "def generate_output_data(ref_data, validated_data, symbol_data, files_per_day, date):\n",
    "    spark = validated_data.sparkSession  \n",
    "\n",
    "    (\n",
    "        currency_data, exchange_data, order_types_data, sides_data,\n",
    "        transaction_types_data, order_statuses_data, mics_data\n",
    "    ) = ref_data\n",
    "    \n",
    "    # Join validated_data with reference tables\n",
    "    output_data = (\n",
    "        validated_data\n",
    "        .join(transaction_types_data.selectExpr(\"transaction_type\", \"transaction_type_id\"), \"transaction_type\", \"left\")\n",
    "        .join(mics_data.selectExpr(\"mic_code\", \"mic_id\"), \"mic_code\", \"left\")\n",
    "        .join(order_statuses_data.selectExpr(\"order_status\", \"order_status_id\"), \"order_status\", \"left\")\n",
    "        .join(sides_data.selectExpr(\"side\", \"side_id\"), \"side\", \"left\")\n",
    "        .join(order_types_data.selectExpr(\"order_type_name as order_type\", \"order_type_id\"), \"order_type\", \"left\")\n",
    "        .join(currency_data.selectExpr(\"currency_name\", \"currency_id\"), \"currency_name\", \"left\")\n",
    "        .join(exchange_data.selectExpr(\"exchange_code\", \"exchange_id\"), \"exchange_code\", \"left\")\n",
    "        .join(symbol_data.selectExpr(\"symbol\", \"listing_internal_id\"), \"symbol\", \"left\")\n",
    "    )\n",
    "\n",
    "    # Define column order\n",
    "    column_order = [\n",
    "        \"transaction_id\", \"transaction_parent_id\",\n",
    "        \"transaction_timestamp\", \"transaction_type_id\", \"mic_id\",\n",
    "        \"order_status_id\", \"side_id\", \"order_type_id\", \"symbol\",\n",
    "        \"isin\", \"price\", \"quantity\", \"trader_id\", \"broker_id\",\n",
    "        \"exchange_id\", \"currency_id\", \"listing_internal_id\",\n",
    "        \"creation_time\", \"last_update_time\", \"validation_flag\"\n",
    "    ]\n",
    "\n",
    "    # Add timestamps\n",
    "    output_data = (\n",
    "        output_data\n",
    "        .withColumn(\"creation_time\", current_timestamp())\n",
    "        .withColumn(\"last_update_time\", current_timestamp())\n",
    "        .select(*column_order)\n",
    "    )\n",
    "\n",
    "    # Repartition data\n",
    "    output_data = output_data.repartition(files_per_day)\n",
    "\n",
    "    # Define output path\n",
    "    output_path = f\"output_data\"\n",
    "    \n",
    "    try:\n",
    "        # Write the output data to the specified path\n",
    "        output_data.write \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"compression\", \"snappy\") \\\n",
    "            .parquet(output_path)\n",
    "        \n",
    "\n",
    "        print(\"Output data written successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error writing output data: {e}\")\n",
    "\n",
    "    return output_data\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Reference data\n",
    "ref_data = (\n",
    "    currency_data,\n",
    "    exchange_data,\n",
    "    order_types_data,\n",
    "    sides_data,\n",
    "    transaction_types_data,\n",
    "    order_statuses_data,\n",
    "    mics_data,\n",
    ")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(\"output_data\", exist_ok=True)\n",
    "\n",
    "# Generate output data\n",
    "output_data = generate_output_data(ref_data, validated_data, symbol_data, 20, date)\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate execution time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution Time for output data generation: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Meet:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[2]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Takumi ETL</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x299bd9c9890>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# import os\n",
    "# import time\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# s3 = boto3.client(\n",
    "#     's3',\n",
    "#     aws_access_key_id=\"\",\n",
    "#     aws_secret_access_key=\"\",\n",
    "#     region_name=\"\"\n",
    "# )\n",
    "\n",
    "# local_folder = \"output_data\"\n",
    "# s3_bucket = \"output-data-dump-bucket\"\n",
    "# s3_prefix = \"parquet/\"  \n",
    "\n",
    "# for file in os.listdir(local_folder):\n",
    "#     if file.endswith(\".snappy.parquet\") and not file.startswith((\"_\", \".\")):  \n",
    "#         s3.upload_file(os.path.join(local_folder, file), s3_bucket, s3_prefix + file)\n",
    "#         print(f\"Uploaded {file} to s3://{s3_bucket}/{s3_prefix}\")\n",
    "\n",
    "# end = time.time()\n",
    "# execution_time = end - start\n",
    "# print(f\"Execution time for dumping files in s3 bucket: {execution_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcc5501f-ad2e-4f1a-8c8b-58f599abe7d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# print(f\"Column Count: {len(output_data.columns)}\")\n",
    "\n",
    "# from pyspark.sql.functions import col,sum\n",
    "# output_data.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in output_data.columns]).show()\n",
    "\n",
    "# output_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8194ae4f-a0b8-4222-ad08-fd549126b667",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zgfTOTG5vQWo",
    "outputId": "54e4a3a0-8c19-4767-d057-14c46f27ced8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "+--------------+---------------------+----------------+---------------+\n",
      "|transaction_id|transaction_timestamp|transaction_type|validation_flag|\n",
      "+--------------+---------------------+----------------+---------------+\n",
      "+--------------+---------------------+----------------+---------------+\n",
      "\n",
      "0\n",
      "+--------------+---------------------+-------------------+---------------+\n",
      "|transaction_id|transaction_timestamp|transaction_type_id|validation_flag|\n",
      "+--------------+---------------------+-------------------+---------------+\n",
      "+--------------+---------------------+-------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Select a sample of rows from validated_data\n",
    "sample_rows = validated_data.sample(withReplacement=False, fraction=0.0001).limit(5)\n",
    "print(sample_rows.count())\n",
    "# display(sample_rows.limit(10))\n",
    "sample_rows.select(\"transaction_id\",\"transaction_timestamp\",\"transaction_type\",\"validation_flag\").orderBy(\"transaction_timestamp\").show()\n",
    "\n",
    "# Extract transaction IDs from the sampled rows for filtering output_data\n",
    "sample_transaction_ids = [row[\"transaction_id\"] for row in sample_rows.collect()]\n",
    "\n",
    "# Filter output_data based on selected transaction IDs\n",
    "filtered_output_data = output_data.filter(col(\"transaction_id\").isin(sample_transaction_ids))\n",
    "print(filtered_output_data.count())\n",
    "# display(filtered_output_data.limit(10))\n",
    "filtered_output_data.select(\"transaction_id\",\"transaction_timestamp\",\"transaction_type_id\",\"validation_flag\").orderBy(\"transaction_timestamp\").show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pyspark_Datagenerator",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
